{"cells":[{"cell_type":"markdown","metadata":{"id":"9PqL6-o-lqcO"},"source":["### Libraries and data loading"]},{"cell_type":"code","execution_count":1,"metadata":{"collapsed":true,"id":"D7j1NOeLlqcS","executionInfo":{"status":"ok","timestamp":1684491132557,"user_tz":-120,"elapsed":5823,"user":{"displayName":"ARNE BERRESHEIM","userId":"06065091921129973079"}}},"outputs":[],"source":["# import libraries\n","#from google.colab import drive\n","import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import time\n","import datetime\n","import itertools\n","import h5py\n","import matplotlib.dates as mdates\n","\n","%matplotlib inline"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"6Hot-v3ulqcT","executionInfo":{"status":"ok","timestamp":1684491132558,"user_tz":-120,"elapsed":13,"user":{"displayName":"ARNE BERRESHEIM","userId":"06065091921129973079"}},"outputId":"47f07038-5f84-4a17-f5ae-18d0d0cb2716","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensorflow version: 2.12.0\n","available gpus: []\n","available cpus: 2\n"]}],"source":["# check tensorflow version\n","print(\"tensorflow version:\", tf.__version__)\n","# check available gpu\n","gpus =  tf.config.list_physical_devices('GPU')\n","print(\"available gpus:\", gpus)\n","# limit the gpu usage, prevent it from allocating all gpu memory for a simple model\n","for gpu in gpus:\n","    tf.config.experimental.set_memory_growth(gpu, True)\n","# check number of cpus available\n","print(\"available cpus:\", os.cpu_count())"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"6XBbzH9blqcU","executionInfo":{"status":"ok","timestamp":1684491158994,"user_tz":-120,"elapsed":26443,"user":{"displayName":"ARNE BERRESHEIM","userId":"06065091921129973079"}},"outputId":"c54b4316-1cf0-4c4f-8587-4596b1bbf316","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","data_folder: /content/drive/MyDrive/Stanford-solar-forecasting-dataset/data\n","data_path: /content/drive/MyDrive/Stanford-solar-forecasting-dataset/data/2017_2019_images_pv_processed.hdf5\n","output_folder: /content/drive/MyDrive/Stanford-solar-forecasting-dataset/model_output/SUNSET_nowcast+image_ill\n"]}],"source":["# define the data location and load data\n","cwd = os.getcwd()\n","pardir = os.path.dirname(cwd)\n","\n","#### commented for google drive\n","#drive.mount('/content/drive')\n","#pardir = \"/content/drive/MyDrive/Stanford-solar-forecasting-dataset/\"\n","\n","data_folder = os.path.join(pardir,\"data\")   #,\"data_nowcast\")\n","data_path = os.path.join(data_folder, \"2017_2019_images_pv_processed.hdf5\")     #nowcast_dataset\n","\n","# !change model name for different models!\n","model_name = 'SUNSET_nowcast+image_ill'\n","output_folder = os.path.join(pardir,\"model_output\", model_name)\n","if os.path.isdir(output_folder)==False:\n","    os.makedirs(output_folder)\n","\n","print(\"data_folder:\", data_folder)\n","print(\"data_path:\", data_path)\n","print(\"output_folder:\", output_folder)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"8EkqTEzIlqcV","executionInfo":{"status":"ok","timestamp":1684491162228,"user_tz":-120,"elapsed":2927,"user":{"displayName":"ARNE BERRESHEIM","userId":"06065091921129973079"}},"outputId":"df24068b-196e-4cde-a413-5745266d7e64","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["<HDF5 group \"/clearskylibrary\" (2 members)>\n","<HDF5 dataset \"csl_images\": shape (574, 64, 64, 3), type \"<f4\">\n","<HDF5 dataset \"csl_sun_pos\": shape (574, 2), type \"<f4\">\n","<HDF5 group \"/test\" (7 members)>\n","<HDF5 dataset \"cloud_seg\": shape (14003, 64, 64), type \"<f4\">\n","<HDF5 dataset \"image_ill\": shape (14003, 1), type \"<f4\">\n","<HDF5 dataset \"images_log\": shape (14003, 64, 64, 3), type \"|u1\">\n","<HDF5 dataset \"pv_log\": shape (14003,), type \"<f8\">\n","<HDF5 dataset \"sampi\": shape (14003, 1), type \"<f4\">\n","<HDF5 dataset \"sun_pos\": shape (14003, 2), type \"<f4\">\n","<HDF5 dataset \"sun_seg\": shape (14003, 64, 64), type \"<f4\">\n","<HDF5 group \"/trainval\" (7 members)>\n","<HDF5 dataset \"cloud_seg\": shape (349372, 64, 64), type \"<f4\">\n","<HDF5 dataset \"image_ill\": shape (349372, 1), type \"<f4\">\n","<HDF5 dataset \"images_log\": shape (349372, 64, 64, 3), type \"|u1\">\n","<HDF5 dataset \"pv_log\": shape (349372,), type \"<f8\">\n","<HDF5 dataset \"sampi\": shape (349372, 1), type \"<f4\">\n","<HDF5 dataset \"sun_pos\": shape (349372, 2), type \"<f4\">\n","<HDF5 dataset \"sun_seg\": shape (349372, 64, 64), type \"<f4\">\n"]}],"source":["# generate handler for the hdf5 data\n","forecast_dataset = h5py.File(data_path, 'r')\n","\n","# show structure of the hdf5 data\n","def get_all(name):\n","    if name!=None:\n","        print(forecast_dataset[name])\n","    \n","forecast_dataset.visit(get_all)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"udjbX9kMlqcV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684491206386,"user_tz":-120,"elapsed":44165,"user":{"displayName":"ARNE BERRESHEIM","userId":"06065091921129973079"}},"outputId":"496da726-50d6-4923-de79-b88a87d0fe95"},"outputs":[{"output_type":"stream","name":"stdout","text":["--------------------------------------------------\n","image side length: 64\n","number of color channels: 3\n","input image dimension: [64, 64, 3]\n","image_ill dimension: 1\n","times_trainval.shape: (349372,)\n","--------------------------------------------------\n","data reading start...\n","batch 1 samples: 0 to 69873, 20.00% finished, processing time 6.72s\n","batch 2 samples: 69874 to 139747, 40.00% finished, processing time 10.43s\n","batch 3 samples: 139748 to 209621, 60.00% finished, processing time 7.38s\n","batch 4 samples: 209622 to 279495, 80.00% finished, processing time 9.79s\n","batch 5 samples: 279496 to 349369, 100.00% finished, processing time 7.84s\n","batch 6 samples: 349370 to 349371, 100.00% finished, processing time 0.00s\n"]}],"source":["print('-'*50)\n","# get the input dimension for constructing the model\n","img_side_len = forecast_dataset['trainval']['images_log'].shape[1]\n","num_color_channel = forecast_dataset['trainval']['images_log'].shape[3]\n","image_input_dim = [img_side_len,img_side_len,num_color_channel]\n","#MYEDIT: getting the dim of the image_ill\n","image_ill_dim = forecast_dataset['trainval']['image_ill'].shape[1]\n","\n","print(\"image side length:\", img_side_len)\n","print(\"number of color channels:\", num_color_channel)\n","print(\"input image dimension:\", image_input_dim)\n","#MYEDIT\n","print(\"image_ill dimension:\", image_ill_dim)\n","\n","# load time stamps into the memory\n","times_trainval = np.load(os.path.join(data_folder,\"times_trainval.npy\"),allow_pickle=True)\n","print(\"times_trainval.shape:\", times_trainval.shape)\n","\n","# read through the dataset once in order to cache it but not store it into the memory\n","## read the data by batch\n","num_samples = len(times_trainval)\n","batch_size = num_samples//5\n","indices = np.arange(num_samples)\n","print('-'*50)\n","print('data reading start...')\n","for i in range(int(num_samples / batch_size) + 1):\n","    start_time = time.time()\n","    start_idx = (i * batch_size) % num_samples\n","    idxs = indices[start_idx:start_idx + batch_size]\n","    _ = forecast_dataset['trainval']['images_log'][idxs]\n","    _ = forecast_dataset['trainval']['pv_log'][idxs]\n","    _ = forecast_dataset['trainval']['image_ill'][idxs]\n","    end_time = time.time()\n","    print(\"batch {0} samples: {1} to {2}, {3:.2f}% finished, processing time {4:.2f}s\"\n","          .format(i+1, idxs[0],idxs[-1],(idxs[-1]/num_samples)*100,(end_time-start_time)))\n","\n","# temporially close the dataset, will use \"with\" statement to open it when we use it\n","forecast_dataset.close()"]},{"cell_type":"markdown","metadata":{"id":"9rpIrln6lqcW"},"source":["### Input data pipeline helper functions"]},{"cell_type":"code","execution_count":6,"metadata":{"collapsed":true,"id":"JkzLeY2MlqcW","executionInfo":{"status":"ok","timestamp":1684491206386,"user_tz":-120,"elapsed":24,"user":{"displayName":"ARNE BERRESHEIM","userId":"06065091921129973079"}}},"outputs":[],"source":["# day block shuffling of the time stamps, and return shuffled indices\n","def day_block_shuffle(times_trainval):\n","    \n","    # Only keep the date of each time point\n","    dates_trainval = np.zeros_like(times_trainval, dtype=datetime.date)\n","    for i in range(len(times_trainval)):\n","        dates_trainval[i] = times_trainval[i].date()\n","\n","    # Chop the indices into blocks, so that each block contains the indices of the same day\n","    unique_dates = np.unique(dates_trainval)\n","    blocks = []\n","    for i in range(len(unique_dates)):\n","        blocks.append(np.where(dates_trainval == unique_dates[i])[0])\n","\n","    # shuffle the blocks, and chain it back together\n","    np.random.seed(1)\n","    np.random.shuffle(blocks)\n","    shuffled_indices = np.asarray(list(itertools.chain.from_iterable(blocks)))\n","\n","    return shuffled_indices"]},{"cell_type":"code","execution_count":7,"metadata":{"collapsed":true,"id":"0BagsEwKlqcX","executionInfo":{"status":"ok","timestamp":1684491206387,"user_tz":-120,"elapsed":23,"user":{"displayName":"ARNE BERRESHEIM","userId":"06065091921129973079"}}},"outputs":[],"source":["# a cross validation generator function for spliting the dayblock shuffled indices into training and validation\n","def cv_split(split_data, fold_index, num_fold):\n","    '''\n","    input:\n","    split_data: the dayblock shuffled indices to be splitted\n","    fold_index: the ith fold chosen as the validation, used for generating the seed for random shuffling\n","    num_fold: N-fold cross validation\n","    output:\n","    data_train: the train data indices\n","    data_val: the validation data indices\n","    '''\n","    # randomly divides into a training set and a validation set\n","    num_samples = len(split_data)\n","    indices = np.arange(num_samples)\n","\n","    # finding training and validation indices\n","    val_mask = np.zeros(len(indices), dtype=bool)\n","    val_mask[int(fold_index / num_fold * num_samples):int((fold_index + 1) / num_fold * num_samples)] = True\n","    val_indices = indices[val_mask]\n","    train_indices = indices[np.logical_not(val_mask)]\n","\n","    # shuffle indices\n","    np.random.seed(fold_index)\n","    np.random.shuffle(train_indices)\n","    np.random.shuffle(val_indices)\n","    \n","    data_train = split_data[train_indices]\n","    data_val = split_data[val_indices]\n","\n","    return data_train,data_val"]},{"cell_type":"code","execution_count":8,"metadata":{"collapsed":true,"id":"Vwrj62GalqcY","executionInfo":{"status":"ok","timestamp":1684491206387,"user_tz":-120,"elapsed":22,"user":{"displayName":"ARNE BERRESHEIM","userId":"06065091921129973079"}}},"outputs":[],"source":["# a mapping function from the indices to the corresponding features and labels \n","def data_loader(hdf5_data_path,sample_idx,batch_size=256):\n","    '''\n","    input:\n","    hdf5_data_path: path to hdf5 data file\n","    sample_idx: \n","        for training and validation:\n","            dayblock shuffled indices with cross-validation split into training and validation\n","            either training or validation indices will be input\n","        for testing: the indices are not shuffled\n","    is_trainval: a flag, True for trainig and validation\n","    output:\n","    dataset: dataset for training, validation\n","    '''\n","\n","    def mapping_func_py(hdf5_data_path,sample_idx):\n","        '''\n","        mapping indices to corresponding images and pviance data in hdf5 (python expression)\n","        '''\n","        # convert EagerTensor to str or numpy array\n","        hdf5_data_path = hdf5_data_path.numpy().decode() \n","        # sort the sample indices as hdf5 requires increasing order index for data retrieval\n","        sample_idx = sorted(sample_idx.numpy())\n","\n","        with h5py.File(hdf5_data_path,'r') as f:\n","\n","            # read in the data\n","            images_log = f['trainval']['images_log'][sample_idx]\n","            pv_log = f['trainval']['pv_log'][sample_idx]\n","            #MYEDIT: retrieve image_ill information\n","            image_ill = f['trainval']['image_ill'][sample_idx]\n","\n","            # normalize image data to [0,1]\n","            images_log = tf.image.convert_image_dtype(images_log, tf.float32)\n","            \n","            # convert pv data to tf.tensor\n","            pv_log = tf.convert_to_tensor(pv_log, dtype=tf.float32)\n","\n","            #MYEDIT: convert image_ill data to tf.tensor\n","            image_ill = tf.convert_to_tensor(image_ill, dtype=tf.float32)\n","            \n","            return images_log, pv_log, image_ill\n","\n","    def mapping_func_tf(hdf5_data_path,sample_idx):\n","        '''\n","        a wrapper mapping function to get the nested data structure \n","        the output type of tf.py_function cannot be a nested sequence when using a tf.py_function with the tf.data API\n","        '''\n","        images_log, pv_log, image_ill = tf.py_function(func=mapping_func_py,\n","                                                           inp=[hdf5_data_path, sample_idx], \n","                                                           Tout=(tf.float32, tf.float32, tf.float32))\n","        #MYEDIT: multiple input requires brackets\n","        return (images_log, image_ill), pv_log\n","    \n","    \n","    # create the indices dataset\n","    idx_ds = tf.data.Dataset.from_tensor_slices(sample_idx)\n","    # shuffle and batch the indices\n","    idx_ds = idx_ds.shuffle(buffer_size = idx_ds.cardinality().numpy(),seed=0)\n","    idx_ds = idx_ds.batch(batch_size).repeat().prefetch(tf.data.experimental.AUTOTUNE)\n","    \n","    # indices dataset mapping to images and pviance data\n","    # returning dataset with the following nested structure: (images_log, pv_log), pv_pred\n","    dataset = idx_ds.map(lambda x: mapping_func_tf(hdf5_data_path,x),\n","                         num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","    \n","    return dataset"]},{"cell_type":"markdown","metadata":{"id":"vQV7tiuBlqcY"},"source":["### Model architecture"]},{"cell_type":"code","execution_count":9,"metadata":{"collapsed":true,"id":"Y2xJpXpalqcY","executionInfo":{"status":"ok","timestamp":1684491206387,"user_tz":-120,"elapsed":21,"user":{"displayName":"ARNE BERRESHEIM","userId":"06065091921129973079"}}},"outputs":[],"source":["# define model characteristics\n","num_filters = 24\n","kernel_size = [3,3]\n","pool_size = [2,2]\n","strides = 2\n","dense_size = 1024\n","drop_rate = 0.4\n","\n","# define training time parameters\n","num_epochs = 200 #(The maximum epoches set to 200 and there might be early stopping depends on validation loss)\n","num_fold = 10 # 10-fold cross-validation\n","batch_size = 256\n","learning_rate = 3e-06"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"P_4D4IATlqcZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684491208014,"user_tz":-120,"elapsed":1647,"user":{"displayName":"ARNE BERRESHEIM","userId":"06065091921129973079"}},"outputId":"04da87de-7d3c-460f-f717-8c1119fb0032"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 64, 64, 3)]  0           []                               \n","                                                                                                  \n"," conv2d (Conv2D)                (None, 64, 64, 24)   672         ['input_1[0][0]']                \n","                                                                                                  \n"," batch_normalization (BatchNorm  (None, 64, 64, 24)  96          ['conv2d[0][0]']                 \n"," alization)                                                                                       \n","                                                                                                  \n"," max_pooling2d (MaxPooling2D)   (None, 32, 32, 24)   0           ['batch_normalization[0][0]']    \n","                                                                                                  \n"," conv2d_1 (Conv2D)              (None, 32, 32, 48)   10416       ['max_pooling2d[0][0]']          \n","                                                                                                  \n"," batch_normalization_1 (BatchNo  (None, 32, 32, 48)  192         ['conv2d_1[0][0]']               \n"," rmalization)                                                                                     \n","                                                                                                  \n"," max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 48)  0           ['batch_normalization_1[0][0]']  \n","                                                                                                  \n"," flatten (Flatten)              (None, 12288)        0           ['max_pooling2d_1[0][0]']        \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, 1)]          0           []                               \n","                                                                                                  \n"," concatenate (Concatenate)      (None, 12289)        0           ['flatten[0][0]',                \n","                                                                  'input_2[0][0]']                \n","                                                                                                  \n"," dense (Dense)                  (None, 1024)         12584960    ['concatenate[0][0]']            \n","                                                                                                  \n"," dropout (Dropout)              (None, 1024)         0           ['dense[0][0]']                  \n","                                                                                                  \n"," dense_1 (Dense)                (None, 1024)         1049600     ['dropout[0][0]']                \n","                                                                                                  \n"," dropout_1 (Dropout)            (None, 1024)         0           ['dense_1[0][0]']                \n","                                                                                                  \n"," dense_2 (Dense)                (None, 1)            1025        ['dropout_1[0][0]']              \n","                                                                                                  \n","==================================================================================================\n","Total params: 13,646,961\n","Trainable params: 13,646,817\n","Non-trainable params: 144\n","__________________________________________________________________________________________________\n"]}],"source":["# define the model architecture using tf.keras API\n","def sunset_model():\n","    ## input\n","    ### input image logs with shape (64,64,3)\n","    x_in = keras.Input(shape=image_input_dim)\n","    ###MYEDIT: input of the sun position with shape (2)\n","    x2_in = keras.Input(shape=image_ill_dim)\n","\n","    ## 1st convolution block\n","    x = keras.layers.Conv2D(num_filters,kernel_size,padding=\"same\",activation='relu')(x_in)\n","    x = keras.layers.BatchNormalization()(x)\n","    x = keras.layers.MaxPooling2D(pool_size, strides)(x)\n","\n","    ## 2nd convolution block\n","    x = keras.layers.Conv2D(num_filters*2,kernel_size,padding=\"same\",activation='relu')(x)\n","    x = keras.layers.BatchNormalization()(x)\n","    x = keras.layers.MaxPooling2D(pool_size, strides)(x)\n","\n","    ## two fully connected nets\n","    x = keras.layers.Flatten()(x)\n","    #MYEDIT: concatenating the image_ill\n","    x = keras.layers.Concatenate(axis=1)([x, x2_in])\n","\n","    x = keras.layers.Dense(dense_size, activation='relu')(x)\n","    x = keras.layers.Dropout(drop_rate)(x)\n","    x = keras.layers.Dense(dense_size, activation='relu')(x)\n","    x = keras.layers.Dropout(drop_rate)(x)\n","\n","    ## regression to prediction target\n","    y_out = keras.layers.Dense(units=1)(x)\n","\n","    # construct the model MYEDIT: adding the image_ill as input\n","    model = keras.Model(inputs=[x_in, x2_in],outputs=y_out)\n","\n","    return model\n","\n","# show model architecture\n","sunset_model().summary()"]},{"cell_type":"markdown","metadata":{"id":"RcjRts7OlqcZ"},"source":["### Model training and validation"]},{"cell_type":"code","execution_count":11,"metadata":{"scrolled":true,"id":"8nHrkEzVlqcZ","colab":{"base_uri":"https://localhost:8080/","height":443},"executionInfo":{"status":"error","timestamp":1684491237052,"user_tz":-120,"elapsed":29041,"user":{"displayName":"ARNE BERRESHEIM","userId":"06065091921129973079"}},"outputId":"beeae8c5-d5b4-441e-b439-5616780f3422"},"outputs":[{"output_type":"stream","name":"stdout","text":["Repetition 1 model training started ...\n","Epoch 1/200\n","   5/1229 [..............................] - ETA: 59:02 - loss: 182.8896  "]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-379a3c641993>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# training the model and record training and validation loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     history = model.fit(ds_train_batched, epochs=num_epochs, steps_per_epoch=len(indices_train)//batch_size+1,\n\u001b[0m\u001b[1;32m     39\u001b[0m                                \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearlystop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_val_batched\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                               validation_steps=len(indices_val)//batch_size+1)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# generate dayblock shuffled indices\n","indices_dayblock_shuffled = day_block_shuffle(times_trainval)\n","\n","# initialize loss history list\n","train_loss_hist = []\n","val_loss_hist = []\n","    \n","for i in range(num_fold):\n","    \n","    # construct and compile model for each repetition to reinitialize the model weights\n","    keras.backend.clear_session()\n","    model = sunset_model()\n","    model.compile(optimizer=keras.optimizers.Adam(learning_rate),loss='mse')\n","    \n","    # implementing 10-fold cross-validation\n","    print('Repetition {0} model training started ...'.format(i+1))\n","    \n","    # creating folder for saving model checkpoint\n","    save_directory = os.path.join(output_folder,'repetition_'+str(i+1))\n","    if not os.path.exists(save_directory):\n","        os.makedirs(save_directory)\n","        \n","    # training and validation data preparation\n","    ## generate indices dataset for training and validation\n","    indices_train, indices_val = cv_split(indices_dayblock_shuffled,i,num_fold)\n","    ## load data from dataloader\n","    ds_train_batched = data_loader(data_path,indices_train)\n","    ds_val_batched = data_loader(data_path,indices_val,batch_size=500)\n","\n","    # define callbacks for training\n","    ## early stopping rule: if the validation loss stop decreasing for 5 consecutive epoches\n","    earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n","    ## model check point: save model checkpoint for later use\n","    checkpoint = keras.callbacks.ModelCheckpoint(os.path.join(save_directory,'best_model_repitition_'+str(i+1)+'.h5'), \n","                                monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n","\n","    # training the model and record training and validation loss\n","    history = model.fit(ds_train_batched, epochs=num_epochs, steps_per_epoch=len(indices_train)//batch_size+1,\n","                               verbose=1, callbacks=[earlystop,checkpoint], validation_data=ds_val_batched,\n","                              validation_steps=len(indices_val)//batch_size+1)\n","    train_loss_hist.append(history.history['loss'])\n","    val_loss_hist.append(history.history['val_loss'])\n","    \n","    # saving model training and validation loss history\n","    np.save(os.path.join(output_folder,'train_loss_hist.npy'),train_loss_hist)\n","    np.save(os.path.join(output_folder,'val_loss_hist.npy'),val_loss_hist)\n","\n","    # plot training and validation history\n","    plt.plot(train_loss_hist[i],label='train')\n","    plt.plot(val_loss_hist[i],label='validation')\n","    plt.legend()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JKcld5pdlqca","executionInfo":{"status":"aborted","timestamp":1684491237053,"user_tz":-120,"elapsed":10,"user":{"displayName":"ARNE BERRESHEIM","userId":"06065091921129973079"}}},"outputs":[],"source":["# summary of training and validation results\n","best_train_loss_MSE = np.zeros(num_fold)\n","best_val_loss_MSE = np.zeros(num_fold)\n","\n","for i in range(num_fold):\n","    best_val_loss_MSE[i] = np.min(val_loss_hist[i])\n","    idx = np.argmin(val_loss_hist[i])\n","    best_train_loss_MSE[i] = train_loss_hist[i][idx]\n","    print('Model {0}  -- train loss: {1:.2f}, validation loss: {2:.2f} (RMSE)'.format(i+1, np.sqrt(best_train_loss_MSE[i]), np.sqrt(best_val_loss_MSE[i])))\n","print('The mean train loss (RMSE) for all models is {0:.2f}'.format(np.mean(np.sqrt(best_train_loss_MSE))))\n","print('The mean validation loss (RMSE) for all models is {0:.2f}'.format(np.mean(np.sqrt(best_val_loss_MSE))))"]},{"cell_type":"markdown","metadata":{"id":"HfaFBpsklqca"},"source":["### Model testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nbb8AhCXlqca","executionInfo":{"status":"aborted","timestamp":1684491237054,"user_tz":-120,"elapsed":10,"user":{"displayName":"ARNE BERRESHEIM","userId":"06065091921129973079"}}},"outputs":[],"source":["# load testing data\n","times_test = np.load(os.path.join(data_folder,\"times_test.npy\"),allow_pickle=True)\n","print(\"times_test.shape:\", times_test.shape)\n","\n","with h5py.File(data_path,'r') as f:\n","\n","    # read in the data\n","    images_log_test = f['test']['images_log'][...]\n","    pv_log_test = f['test']['pv_log'][...]\n","    image_ill_log_test = f['test']['image_ill'][...]\n","\n","# process image data\n","images_log_test = (images_log_test/255.0).astype('float32')\n","pv_log_test = pv_log_test.astype('float32')\n","\n","print(\"images_log_test.shape:\",images_log_test.shape)\n","print(\"pv_log_test.shape:\",pv_log_test.shape)\n","print('image_ill_log_test.shape', image_ill_log_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"ZB5bQX4ilqcb","executionInfo":{"status":"aborted","timestamp":1684491237054,"user_tz":-120,"elapsed":10,"user":{"displayName":"ARNE BERRESHEIM","userId":"06065091921129973079"}}},"outputs":[],"source":["# evaluate model on the test set and generate predictions\n","loss = np.zeros((num_fold,len(times_test)))\n","prediction = np.zeros((num_fold,len(times_test)))\n","\n","for i in range(num_fold):\n","    # define model path\n","    print(\"loading repetition {0} model ...\".format(i+1))\n","    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n","    # load the trained model\n","    model = keras.models.load_model(model_path)\n","    \n","    # model evaluation\n","    print(\"evaluating performance for the model\".format(i+1))\n","    loss[i] = model.evaluate(x=[images_log_test, image_ill_log_test], y=pv_log_test, batch_size=200, verbose=1)\n","    \n","    # generate prediction\n","    print(\"generating predictions for the model\".format(i+1))\n","    prediction[i] = np.squeeze(model.predict([images_log_test, image_ill_log_test], batch_size=200, verbose=1))\n","\n","# saving predictions from each model\n","np.save(os.path.join(output_folder,'test_predictions.npy'),prediction)\n","\n","# using the ensemble mean of the 10 models as the final prediction \n","print('-'*50)\n","print(\"model ensembling ...\")\n","prediction_ensemble = np.mean(prediction,axis=0)\n","loss_ensemble = np.sqrt(np.mean((prediction_ensemble-pv_log_test)**2))\n","print(\"the test set RMSE is {0:.3f} for the ensemble model\".format(loss_ensemble))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Hf22bUvlqcb","executionInfo":{"status":"aborted","timestamp":1684491237055,"user_tz":-120,"elapsed":10,"user":{"displayName":"ARNE BERRESHEIM","userId":"06065091921129973079"}}},"outputs":[],"source":["# formulate sunny and cloudy test days\n","sunny_dates = [(2017,9,15),(2017,10,6),(2017,10,22),\n","               (2018,2,16),(2018,6,12),(2018,6,23),\n","               (2019,1,25),(2019,6,23),(2019,7,14),(2019,10,14)]\n","cloudy_dates = [(2017,6,24),(2017,9,20),(2017,10,11),\n","                (2018,1,25),(2018,3,9),(2018,10,4),\n","                (2019,5,27),(2019,6,28),(2019,8,10),(2019,10,19)]\n","\n","sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n","cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n","\n","dates_test = np.asarray([times.date() for times in times_test])\n","\n","## generate mask for the sunny days\n","mask = np.zeros(len(pv_log_test),dtype=bool)\n","for i in sunny_dates_test:\n","    mask[np.where(dates_test==i)[0]]=1\n","\n","## apply the mask to the dataset\n","times_test_sunny = times_test[mask]\n","pv_log_test_sunny = pv_log_test[mask]\n","images_log_test_sunny = images_log_test[mask]\n","image_ill_log_test_sunny = image_ill_log_test[mask]\n","prediction_ensemble_sunny = prediction_ensemble[mask]\n","print(\"times_test_sunny.shape:\",times_test_sunny.shape)\n","\n","times_test_cloudy = times_test[~mask]\n","pv_log_test_cloudy = pv_log_test[~mask]\n","images_log_test_cloudy = images_log_test[~mask]\n","image_ill_log_test_cloudy = image_ill_log_test[~mask]\n","prediction_ensemble_cloudy = prediction_ensemble[~mask]\n","print(\"times_test_cloudy.shape:\",times_test_cloudy.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8aaJuQi_lqcb","executionInfo":{"status":"aborted","timestamp":1684491237055,"user_tz":-120,"elapsed":10,"user":{"displayName":"ARNE BERRESHEIM","userId":"06065091921129973079"}}},"outputs":[],"source":["# RMSE for sunny and cloudy days individually\n","rmse_sunny = np.sqrt(np.mean(np.square((prediction_ensemble_sunny-pv_log_test_sunny))))\n","rmse_cloudy = np.sqrt(np.mean(np.square((prediction_ensemble_cloudy-pv_log_test_cloudy))))\n","rmse_overall = np.sqrt((rmse_sunny**2*len(pv_log_test_sunny)+rmse_cloudy**2*len(pv_log_test_cloudy))/(len(pv_log_test)))\n","\n","print(\"test set sunny days RMSE: {0:.3f}\".format(rmse_sunny))\n","print(\"test set cloudy days RMSE: {0:.3f}\".format(rmse_cloudy))\n","print(\"test set overall RMSE: {0:.3f}\".format(rmse_overall))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pP_rtKF8lqcb","executionInfo":{"status":"aborted","timestamp":1684491237055,"user_tz":-120,"elapsed":10,"user":{"displayName":"ARNE BERRESHEIM","userId":"06065091921129973079"}}},"outputs":[],"source":["# MAE for sunny and cloudy days individually\n","mae_sunny = np.mean(np.abs((prediction_ensemble_sunny-pv_log_test_sunny)))\n","mae_cloudy = np.mean(np.abs((prediction_ensemble_cloudy-pv_log_test_cloudy)))\n","mae_overall = (mae_cloudy*len(pv_log_test_cloudy) + mae_sunny*len(pv_log_test_sunny))/(len(pv_log_test))\n","\n","print(\"test set sunny days MAE: {0:.3f}\".format(mae_sunny))\n","print(\"test set cloudy days MAE: {0:.3f}\".format(mae_cloudy))\n","print(\"test set overall MAE: {0:.3f}\".format(mae_overall))"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"IqmWWdSqlqcc","executionInfo":{"status":"aborted","timestamp":1684491237056,"user_tz":-120,"elapsed":10,"user":{"displayName":"ARNE BERRESHEIM","userId":"06065091921129973079"}}},"outputs":[],"source":["# visualization of nowcast predictions\n","dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n","hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n","\n","f,axarr = plt.subplots(10,2,sharex=False, sharey = True)\n","xfmt = mdates.DateFormatter('%H')\n","fmt_date = datetime.date(2000,1,1)\n","\n","blue = '#67AFD2'\n","grey =  '#B6B1A9'\n","\n","for i,date in enumerate(sunny_dates_test):\n","    ax = axarr[i,0]\n","    date_mask = (dates_test == date)\n","    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]] \n","    \n","    rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_ensemble[date_mask]))))\n","    mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_ensemble[date_mask])))\n","    \n","    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color=grey)\n","    ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n","    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth = 1,label = 'SUNSET nowcast',color=blue,markerfacecolor=\"None\")\n","    ax.set_ylabel('PV output (kW)')\n","    ax.xaxis.set_major_formatter(xfmt)\n","    ax.text(0.75,0.85,'Sunny_'+str(i+1), transform=ax.transAxes)\n","    ax.text(0.05,0.75,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\".format(rmse,mae),transform=ax.transAxes)\n","\n","for i,date in enumerate(cloudy_dates_test):\n","    ax = axarr[i,1]\n","    date_mask = (dates_test == date)\n","    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]] \n","    \n","    rmse = np.sqrt(np.mean(np.square((pv_log_test[date_mask]-prediction_ensemble[date_mask]))))\n","    mae = np.mean(np.abs((pv_log_test[date_mask]-prediction_ensemble[date_mask])))\n","    \n","    ax.plot(hours_xaxis, pv_log_test[date_mask], linewidth = 1,color=grey)\n","    ax.fill_between(hours_xaxis, 0, pv_log_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n","    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth = 1,label = 'SUNSET nowcast',color=blue,markerfacecolor=\"None\")\n","    ax.set_ylabel('PV output (kW)')\n","    ax.xaxis.set_major_formatter(xfmt)\n","    ax.text(0.75,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n","    ax.text(0.05,0.75,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\".format(rmse,mae),transform=ax.transAxes)\n","\n","    \n","axarr[0,0].set_ylim(0, 30)\n","axarr[0,0].legend(bbox_to_anchor= [1.15,1.3], loc = 'upper center', ncol = 3)\n","axarr[-1,0].set_xlabel('Hour of day')\n","axarr[-1,1].set_xlabel('Hour of day')\n","\n","f.set_size_inches(10,25)    \n","plt.show()  "]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"dCT84vEklqcc","executionInfo":{"status":"aborted","timestamp":1684491237056,"user_tz":-120,"elapsed":10,"user":{"displayName":"ARNE BERRESHEIM","userId":"06065091921129973079"}}},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.6.8 ('SKIPPD_venv')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"vscode":{"interpreter":{"hash":"074c471e510a42acbf43893a6d8078cb5a0d55f128880afca4e36b6d6e9c9498"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}