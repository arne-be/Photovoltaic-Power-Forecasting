{"cells":[{"cell_type":"markdown","metadata":{"id":"loIHN6y3lruS"},"source":["### Libraries and data loading"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vC6CfG0SlruU"},"outputs":[],"source":["# import libraries\n","#from google.colab import drive\n","import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import time\n","import datetime\n","import itertools\n","import h5py\n","import matplotlib.dates as mdates\n","\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vfs_z95flruV","executionInfo":{"status":"ok","timestamp":1686128798635,"user_tz":-120,"elapsed":235,"user":{"displayName":"ARNE BERRESHEIM","userId":"06065091921129973079"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6f9a9f97-540f-470b-b74f-e34910985c22"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensorflow version: 2.12.0\n","available gpus: []\n","available cpus: 2\n"]}],"source":["# check tensorflow version\n","print(\"tensorflow version:\", tf.__version__)\n","# check available gpu\n","gpus =  tf.config.list_physical_devices('GPU')\n","print(\"available gpus:\", gpus)\n","# limit the gpu usage, prevent it from allocating all gpu memory for a simple model\n","for gpu in gpus:\n","    tf.config.experimental.set_memory_growth(gpu, True)\n","# check number of cpus available\n","print(\"available cpus:\", os.cpu_count())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RtoClXcLlruW","executionInfo":{"status":"ok","timestamp":1686128824940,"user_tz":-120,"elapsed":26310,"user":{"displayName":"ARNE BERRESHEIM","userId":"06065091921129973079"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3a804fc8-d582-4db3-caca-5bcde2c85724"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","data_folder: /content/drive/MyDrive/Stanford-solar-forecasting-dataset/data\n","data_path: /content/drive/MyDrive/Stanford-solar-forecasting-dataset/data/2017_2019_images_pv_processed.hdf5\n","output_folder: /content/drive/MyDrive/Stanford-solar-forecasting-dataset/model_output/SUNSET_forecast_2017_2019_data\n"]}],"source":["# define the data location and load data\n","cwd = os.getcwd()\n","pardir = os.path.dirname(cwd)\n","\n","#### commented for google drive\n","#drive.mount('/content/drive')\n","#pardir = \"/content/drive/MyDrive/Stanford-solar-forecasting-dataset/\"\n","\n","data_folder = os.path.join(pardir,\"data\")#,\"data_forecast\")\n","data_path = os.path.join(data_folder, \"2017_2019_images_pv_processed_forecast.hdf5\") #forecast_dataset\n","\n","# !change model name for different models!\n","model_name = 'SUNSET_forecast'\n","output_folder = os.path.join(pardir,\"model_output\", model_name)\n","if os.path.isdir(output_folder)==False:\n","    os.makedirs(output_folder)\n","\n","print(\"data_folder:\", data_folder)\n","print(\"data_path:\", data_path)\n","print(\"output_folder:\", output_folder)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XqjL_jdklruW","executionInfo":{"status":"ok","timestamp":1686128827874,"user_tz":-120,"elapsed":2721,"user":{"displayName":"ARNE BERRESHEIM","userId":"06065091921129973079"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2216dbad-f285-4ced-e5fc-44a72a9dd20e"},"outputs":[{"output_type":"stream","name":"stdout","text":["<HDF5 group \"/clearskylibrary\" (2 members)>\n","<HDF5 dataset \"csl_images\": shape (574, 64, 64, 3), type \"<f4\">\n","<HDF5 dataset \"csl_sun_pos\": shape (574, 2), type \"<f4\">\n","<HDF5 group \"/test\" (7 members)>\n","<HDF5 dataset \"cloud_seg\": shape (14003, 64, 64), type \"<f4\">\n","<HDF5 dataset \"image_ill\": shape (14003, 1), type \"<f4\">\n","<HDF5 dataset \"images_log\": shape (14003, 64, 64, 3), type \"|u1\">\n","<HDF5 dataset \"pv_log\": shape (14003,), type \"<f8\">\n","<HDF5 dataset \"sampi\": shape (14003, 1), type \"<f4\">\n","<HDF5 dataset \"sun_pos\": shape (14003, 2), type \"<f4\">\n","<HDF5 dataset \"sun_seg\": shape (14003, 64, 64), type \"<f4\">\n","<HDF5 group \"/trainval\" (7 members)>\n","<HDF5 dataset \"cloud_seg\": shape (349372, 64, 64), type \"<f4\">\n","<HDF5 dataset \"image_ill\": shape (349372, 1), type \"<f4\">\n","<HDF5 dataset \"images_log\": shape (349372, 64, 64, 3), type \"|u1\">\n","<HDF5 dataset \"pv_log\": shape (349372,), type \"<f8\">\n","<HDF5 dataset \"sampi\": shape (349372, 1), type \"<f4\">\n","<HDF5 dataset \"sun_pos\": shape (349372, 2), type \"<f4\">\n","<HDF5 dataset \"sun_seg\": shape (349372, 64, 64), type \"<f4\">\n"]}],"source":["# generate handler for the hdf5 data\n","forecast_dataset = h5py.File(data_path, 'r')\n","\n","# show structure of the hdf5 data\n","def get_all(name):\n","    if name!=None:\n","        print(forecast_dataset[name])\n","    \n","forecast_dataset.visit(get_all)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RLt_lNjolruX","executionInfo":{"status":"error","timestamp":1686128827874,"user_tz":-120,"elapsed":14,"user":{"displayName":"ARNE BERRESHEIM","userId":"06065091921129973079"}},"colab":{"base_uri":"https://localhost:8080/","height":276},"outputId":"e2344c2e-fd41-4dfa-b1e7-93605d1efc5c"},"outputs":[{"output_type":"stream","name":"stdout","text":["--------------------------------------------------\n","(349372, 64, 64, 3)\n"]},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-206c2d7d55a6>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimg_side_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforecast_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trainval'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images_log'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnum_log_term\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforecast_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trainval'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images_log'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnum_color_channel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforecast_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trainval'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images_log'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mimage_input_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimg_side_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_side_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_log_term\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum_color_channel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: tuple index out of range"]}],"source":["print('-'*50)\n","# get the input dimension for constructing the model\n","# the input images will be reshaped from (None, 16, 64, 64, 3) to (None, 64, 64, 24)\n","print(forecast_dataset['trainval']['images_log'].shape)\n","img_side_len = forecast_dataset['trainval']['images_log'].shape[2]\n","num_log_term = forecast_dataset['trainval']['images_log'].shape[1]\n","num_color_channel = forecast_dataset['trainval']['images_log'].shape[4]\n","image_input_dim = [img_side_len,img_side_len,num_log_term*num_color_channel]\n","\n","print(\"image side length:\", img_side_len)\n","print(\"number of log terms:\", num_log_term)\n","print(\"number of color channels:\", num_color_channel)\n","print(\"input image dimension:\", image_input_dim)\n","\n","# load time stamps into the memory\n","times_trainval = np.load(os.path.join(data_folder,\"forecast_times_trainval.npy\"),allow_pickle=True)\n","print(\"times_trainval.shape:\", times_trainval.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xyBMN3MflruX"},"outputs":[],"source":["# read through the dataset once in order to cache it but not store it into the memory\n","## read the data by batch\n","num_samples = len(times_trainval)\n","batch_size = num_samples//5\n","indices = np.arange(num_samples)\n","print('-'*50)\n","print('data reading start...')\n","for i in range(num_samples // batch_size):\n","    start_time = time.time()\n","    start_idx = (i * batch_size) % num_samples\n","    if i<num_samples // batch_size-1:\n","        idxs = indices[start_idx:start_idx + batch_size]\n","    else:\n","        idxs = indices[start_idx:]\n","    _ = forecast_dataset['trainval']['images_log'][idxs]\n","    _ = forecast_dataset['trainval']['pv_log'][idxs]\n","    _ = forecast_dataset['trainval']['pv_pred'][idxs]\n","    end_time = time.time()\n","    print(\"batch {0} samples: {1} to {2}, {3:.2f}% finished, processing time {4:.2f}s\"\n","          .format(i+1, idxs[0],idxs[-1],(idxs[-1]/num_samples)*100,(end_time-start_time)))\n","\n","# temporially close the dataset, will use \"with\" statement to open it when we use it\n","forecast_dataset.close()"]},{"cell_type":"markdown","metadata":{"id":"eKJ2FXcZlruX"},"source":["### Input data pipeline helper functions"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"NCBSxYhMlruY"},"outputs":[],"source":["# day block shuffling of the time stamps, and return shuffled indices\n","def day_block_shuffle(times_trainval):\n","    \n","    # Only keep the date of each time point\n","    dates_trainval = np.zeros_like(times_trainval, dtype=datetime.date)\n","    for i in range(len(times_trainval)):\n","        dates_trainval[i] = times_trainval[i].date()\n","\n","    # Chop the indices into blocks, so that each block contains the indices of the same day\n","    unique_dates = np.unique(dates_trainval)\n","    blocks = []\n","    for i in range(len(unique_dates)):\n","        blocks.append(np.where(dates_trainval == unique_dates[i])[0])\n","\n","    # shuffle the blocks, and chain it back together\n","    np.random.seed(1)\n","    np.random.shuffle(blocks)\n","    shuffled_indices = np.asarray(list(itertools.chain.from_iterable(blocks)))\n","\n","    return shuffled_indices"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"SMkgrLVilruZ"},"outputs":[],"source":["# a cross validation generator function for spliting the dayblock shuffled indices into training and validation\n","def cv_split(split_data, fold_index, num_fold):\n","    '''\n","    input:\n","    split_data: the dayblock shuffled indices to be splitted\n","    fold_index: the ith fold chosen as the validation, used for generating the seed for random shuffling\n","    num_fold: N-fold cross validation\n","    output:\n","    data_train: the train data indices\n","    data_val: the validation data indices\n","    '''\n","    # randomly divides into a training set and a validation set\n","    num_samples = len(split_data)\n","    indices = np.arange(num_samples)\n","\n","    # finding training and validation indices\n","    val_mask = np.zeros(len(indices), dtype=bool)\n","    val_mask[int(fold_index / num_fold * num_samples):int((fold_index + 1) / num_fold * num_samples)] = True\n","    val_indices = indices[val_mask]\n","    train_indices = indices[np.logical_not(val_mask)]\n","\n","    # shuffle indices\n","    np.random.seed(fold_index)\n","    np.random.shuffle(train_indices)\n","    np.random.shuffle(val_indices)\n","    \n","    data_train = split_data[train_indices]\n","    data_val = split_data[val_indices]\n","\n","    return data_train,data_val"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"rc0v9KD6lruZ"},"outputs":[],"source":["def process_image(image_data):\n","    '''\n","    image data processing: reshaping and normalization\n","    '''\n","    ## reshape the image tensor from [None,16,64,64,3] to [None,64,64,48]\n","    image_data = tf.transpose(image_data,perm=[0,2,3,1,4])\n","    image_data = tf.reshape(image_data, [image_data.shape[0],image_data.shape[1],image_data.shape[2],-1])\n","\n","    ## normalize the image to [0,1]\n","    image_data = tf.image.convert_image_dtype(image_data, tf.float32)\n","\n","    return image_data"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"k9MCJqTElruZ"},"outputs":[],"source":["# a mapping function from the indices to the corresponding features and labels \n","def data_loader(hdf5_data_path,sample_idx,batch_size=256):\n","    '''\n","    input:\n","    hdf5_data_path: path to hdf5 data file\n","    sample_idx: \n","        for training and validation:\n","            dayblock shuffled indices with cross-validation split into training and validation\n","            either training or validation indices will be input\n","        for testing: the indices are not shuffled\n","    is_trainval: a flag, True for trainig and validation\n","    output:\n","    dataset: dataset for training, validation\n","    '''\n","\n","    def mapping_func_py(hdf5_data_path,sample_idx):\n","        '''\n","        mapping indices to corresponding images and pviance data in hdf5 (python expression)\n","        '''\n","        # convert EagerTensor to str or numpy array\n","        hdf5_data_path = hdf5_data_path.numpy().decode() \n","        # sort the sample indices as hdf5 requires increasing order index for data retrieval\n","        sample_idx = sorted(sample_idx.numpy())\n","\n","        with h5py.File(hdf5_data_path,'r') as f:\n","\n","            # read in the data\n","            images_log = f['trainval']['images_log'][sample_idx]\n","            pv_log = f['trainval']['pv_log'][sample_idx]\n","            pv_pred = f['trainval']['pv_pred'][sample_idx]\n","\n","            # process image data\n","            images_log = process_image(images_log)\n","            \n","            # convert pv data to tf.tensor\n","            pv_log = tf.convert_to_tensor(pv_log, dtype=tf.float32)\n","            pv_pred = tf.convert_to_tensor(pv_pred, dtype=tf.float32)\n","\n","            return images_log, pv_log, pv_pred\n","\n","    def mapping_func_tf(hdf5_data_path,sample_idx):\n","        '''\n","        a wrapper mapping function to get the nested data structure \n","        the output type of tf.py_function cannot be a nested sequence when using a tf.py_function with the tf.data API\n","        '''\n","        images_log, pv_log, pv_pred = tf.py_function(func=mapping_func_py,\n","                                                           inp=[hdf5_data_path, sample_idx], \n","                                                           Tout=(tf.float32, tf.float32, tf.float32))\n","        return (images_log, pv_log), pv_pred\n","    \n","    \n","    # create the indices dataset\n","    idx_ds = tf.data.Dataset.from_tensor_slices(sample_idx)\n","    # shuffle and batch the indices\n","    idx_ds = idx_ds.shuffle(buffer_size = idx_ds.cardinality().numpy(),seed=0)\n","    idx_ds = idx_ds.batch(batch_size).repeat().prefetch(tf.data.experimental.AUTOTUNE)\n","    \n","    # indices dataset mapping to images and pviance data\n","    # returning dataset with the following nested structure: (images_log, pv_log), pv_pred\n","    dataset = idx_ds.map(lambda x: mapping_func_tf(hdf5_data_path,x),\n","                         num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","    \n","    return dataset"]},{"cell_type":"markdown","metadata":{"id":"7R9jUDynlrua"},"source":["### Model architecture"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"li5AnBlglrua"},"outputs":[],"source":["# define model characteristics\n","num_filters = 24\n","kernel_size = [3,3]\n","pool_size = [2,2]\n","strides = 2\n","dense_size = 1024\n","drop_rate = 0.4\n","\n","# define training time parameters\n","num_epochs = 200 #(The maximum epoches set to 200 and there might be early stopping depends on validation loss)\n","num_fold = 10 # 10-fold cross-validation\n","batch_size = 256\n","learning_rate = 3e-06"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XIW-eqBwlrua"},"outputs":[],"source":["# define the model architecture using tf.keras API\n","def sunset_model():\n","    ## input\n","    ### input image logs with shape (64,64,24)\n","    x_in = keras.Input(shape=image_input_dim)\n","    ### input pviance/pv output logs with shape (8)\n","    x2_in = keras.Input(shape=num_log_term)\n","\n","    ## 1st convolution block\n","    x = keras.layers.Conv2D(num_filters,kernel_size,padding=\"same\",activation='relu')(x_in)\n","    x = keras.layers.BatchNormalization()(x)\n","    x = keras.layers.MaxPooling2D(pool_size, strides)(x)\n","\n","    ## 2nd convolution block\n","    x = keras.layers.Conv2D(num_filters*2,kernel_size,padding=\"same\",activation='relu')(x)\n","    x = keras.layers.BatchNormalization()(x)\n","    x = keras.layers.MaxPooling2D(pool_size, strides)(x)\n","\n","    ## two fully connected nets\n","    x = keras.layers.Flatten()(x)\n","    x = keras.layers.Concatenate(axis=1)([x, x2_in])\n","\n","    x = keras.layers.Dense(dense_size, activation='relu')(x)\n","    x = keras.layers.Dropout(drop_rate)(x)\n","    x = keras.layers.Dense(dense_size, activation='relu')(x)\n","    x = keras.layers.Dropout(drop_rate)(x)\n","\n","    ## regression to prediction target\n","    y_out = keras.layers.Dense(units=1)(x)\n","\n","    # construct the model\n","    model = keras.Model(inputs=[x_in, x2_in],outputs=y_out)\n","\n","    return model\n","\n","# show model architecture\n","sunset_model().summary()"]},{"cell_type":"markdown","metadata":{"id":"00q3dLvtlrub"},"source":["### Model training and validation"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"sBwqHMZylrub"},"outputs":[],"source":["# generate dayblock shuffled indices\n","indices_dayblock_shuffled = day_block_shuffle(times_trainval)\n","\n","# initialize loss history list\n","train_loss_hist = []\n","val_loss_hist = []\n","    \n","for i in range(num_fold):\n","    \n","    # construct and compile model for each repetition to reinitialize the model weights\n","    keras.backend.clear_session()\n","    model = sunset_model()\n","    model.compile(optimizer=keras.optimizers.Adam(learning_rate),loss='mse')\n","    \n","    # implementing 10-fold cross-validation\n","    print('Repetition {0} model training started ...'.format(i+1))\n","    \n","    # creating folder for saving model checkpoint\n","    save_directory = os.path.join(output_folder,'repetition_'+str(i+1))\n","    if not os.path.exists(save_directory):\n","        os.makedirs(save_directory)\n","        \n","    # training and validation data preparation\n","    ## generate indices dataset for training and validation\n","    indices_train, indices_val = cv_split(indices_dayblock_shuffled,i,num_fold)\n","    ## load data from dataloader\n","    ds_train_batched = data_loader(data_path,indices_train)\n","    ds_val_batched = data_loader(data_path,indices_val,batch_size=500)\n","\n","    # define callbacks for training\n","    ## early stopping rule: if the validation loss stop decreasing for 5 consecutive epoches\n","    earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n","    ## model check point: save model checkpoint for later use\n","    checkpoint = keras.callbacks.ModelCheckpoint(os.path.join(save_directory,'best_model_repitition_'+str(i+1)+'.h5'), \n","                                monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n","\n","    # training the model and record training and validation loss\n","    history = model.fit(ds_train_batched, epochs=num_epochs, steps_per_epoch=len(indices_train)//batch_size+1,\n","                               verbose=1, callbacks=[earlystop,checkpoint], validation_data=ds_val_batched,\n","                              validation_steps=len(indices_val)//batch_size+1)\n","    train_loss_hist.append(history.history['loss'])\n","    val_loss_hist.append(history.history['val_loss'])\n","\n","    # plot training and validation history\n","    plt.plot(train_loss_hist[i],label='train')\n","    plt.plot(val_loss_hist[i],label='validation')\n","    plt.legend()\n","    plt.show()\n","\n","    # saving model training and validation loss history\n","    np.save(os.path.join(output_folder,'train_loss_hist.npy'),train_loss_hist)\n","    np.save(os.path.join(output_folder,'val_loss_hist.npy'),val_loss_hist)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GcQ0xV_8lrub"},"outputs":[],"source":["# summary of training and validation results\n","best_train_loss_MSE = np.zeros(num_fold)\n","best_val_loss_MSE = np.zeros(num_fold)\n","\n","for i in range(num_fold):\n","    best_val_loss_MSE[i] = np.min(val_loss_hist[i])\n","    idx = np.argmin(val_loss_hist[i])\n","    best_train_loss_MSE[i] = train_loss_hist[i][idx]\n","    print('Model {0}  -- train loss: {1:.2f}, validation loss: {2:.2f} (RMSE)'.format(i+1, np.sqrt(best_train_loss_MSE[i]), np.sqrt(best_val_loss_MSE[i])))\n","print('The mean train loss (RMSE) for all models is {0:.2f}'.format(np.mean(np.sqrt(best_train_loss_MSE))))\n","print('The mean validation loss (RMSE) for all models is {0:.2f}'.format(np.mean(np.sqrt(best_val_loss_MSE))))"]},{"cell_type":"markdown","metadata":{"id":"mujSLF_slruc"},"source":["### Model testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2mL5LxK9lruc"},"outputs":[],"source":["# load testing data\n","times_test = np.load(os.path.join(data_folder,\"times_test.npy\"),allow_pickle=True)\n","print(\"times_test.shape:\", times_test.shape)\n","\n","with h5py.File(data_path,'r') as f:\n","\n","    # read in the data\n","    images_log_test = f['test']['images_log'][...]\n","    pv_log_test = f['test']['pv_log'][...]\n","    pv_pred_test = f['test']['pv_pred'][...]\n","\n","# process image data\n","images_log_test = np.transpose(images_log_test,axes=[0,2,3,1,4])\n","images_log_test = np.reshape(images_log_test, [images_log_test.shape[0],images_log_test.shape[1],images_log_test.shape[2],-1])\n","images_log_test = (images_log_test/255.0).astype('float32')\n","    \n","print(\"images_log_test.shape:\",images_log_test.shape)\n","print(\"pv_log_test.shape:\",pv_log_test.shape)\n","print(\"pv_pred_test.shape:\",pv_pred_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"GP8Sb3bplruc"},"outputs":[],"source":["# evaluate model on the test set and generate predictions\n","loss = np.zeros((num_fold,len(times_test)))\n","prediction = np.zeros((num_fold,len(times_test)))\n","\n","for i in range(num_fold):\n","    # define model path\n","    print(\"loading repetition {0} model ...\".format(i+1))\n","    model_path = os.path.join(output_folder,'repetition_'+str(i+1),'best_model_repitition_'+str(i+1)+'.h5')\n","    # load the trained model\n","    model = keras.models.load_model(model_path)\n","    \n","    # model evaluation\n","    print(\"evaluating performance for the model\".format(i+1))\n","    loss[i] = model.evaluate(x=[images_log_test,pv_log_test], y=pv_pred_test, batch_size=200, verbose=1)\n","    \n","    # generate prediction\n","    print(\"generating predictions for the model\".format(i+1))\n","    prediction[i] = np.squeeze(model.predict([images_log_test,pv_log_test], batch_size=200, verbose=1))\n","\n","# saving predictions from each model\n","np.save(os.path.join(output_folder,'test_predictions.npy'),prediction)\n","\n","# using the ensemble mean of the 10 models as the final prediction \n","print('-'*50)\n","print(\"model ensembling ...\")\n","prediction_ensemble = np.mean(prediction,axis=0)\n","loss_ensemble = np.sqrt(np.mean((prediction_ensemble-pv_pred_test)**2))\n","print(\"the test set RMSE is {0:.3f} for the ensemble model\".format(loss_ensemble))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sLQGx3LHlrud"},"outputs":[],"source":["# formulate sunny and cloudy test days\n","sunny_dates = [(2017,9,15),(2017,10,6),(2017,10,22),\n","               (2018,2,16),(2018,6,12),(2018,6,23),\n","               (2019,1,25),(2019,6,23),(2019,7,14),(2019,10,14)]\n","cloudy_dates = [(2017,6,24),(2017,9,20),(2017,10,11),\n","                (2018,1,25),(2018,3,9),(2018,10,4),\n","                (2019,5,27),(2019,6,28),(2019,8,10),(2019,10,19)]\n","sunny_dates_test = [datetime.date(day[0],day[1],day[2]) for day in sunny_dates]\n","cloudy_dates_test = [datetime.date(day[0],day[1],day[2]) for day in cloudy_dates]\n","\n","dates_test = np.asarray([times.date() for times in times_test])\n","\n","## generate mask for the sunny days\n","mask = np.zeros(len(pv_pred_test),dtype=bool)\n","for i in sunny_dates_test:\n","    mask[np.where(dates_test==i)[0]]=1\n","\n","## apply the mask to the dataset\n","times_test_sunny = times_test[mask]\n","pv_pred_test_sunny = pv_pred_test[mask]\n","pv_log_test_sunny = pv_log_test[mask]\n","prediction_ensemble_sunny = prediction_ensemble[mask]\n","print(\"times_test_sunny.shape:\",times_test_sunny.shape)\n","\n","times_test_cloudy = times_test[~mask]\n","pv_pred_test_cloudy = pv_pred_test[~mask]\n","pv_log_test_cloudy = pv_log_test[~mask]\n","prediction_ensemble_cloudy = prediction_ensemble[~mask]\n","print(\"times_test_cloudy.shape:\",times_test_cloudy.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IiDUp_eOlrud"},"outputs":[],"source":["# RMSE for sunny and cloudy days individually\n","rmse_sunny = np.sqrt(np.mean(np.square((prediction_ensemble_sunny-pv_pred_test_sunny))))\n","rmse_cloudy = np.sqrt(np.mean(np.square((prediction_ensemble_cloudy-pv_pred_test_cloudy))))\n","rmse_overall = np.sqrt((rmse_sunny**2*len(pv_pred_test_sunny)+rmse_cloudy**2*len(pv_pred_test_cloudy))/(len(pv_pred_test)))\n","\n","print(\"test set sunny days RMSE: {0:.3f}\".format(rmse_sunny))\n","print(\"test set cloudy days RMSE: {0:.3f}\".format(rmse_cloudy))\n","print(\"test set overall RMSE: {0:.3f}\".format(rmse_overall))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3dh7RAKDlrud"},"outputs":[],"source":["# MAE for sunny and cloudy days individually\n","mae_sunny = np.mean(np.abs((prediction_ensemble_sunny-pv_pred_test_sunny)))\n","mae_cloudy = np.mean(np.abs((prediction_ensemble_cloudy-pv_pred_test_cloudy)))\n","mae_overall = (mae_cloudy*len(pv_pred_test_cloudy) + mae_sunny*len(pv_pred_test_sunny))/(len(pv_pred_test))\n","\n","print(\"test set sunny days MAE: {0:.3f}\".format(mae_sunny))\n","print(\"test set cloudy days MAE: {0:.3f}\".format(mae_cloudy))\n","print(\"test set overall MAE: {0:.3f}\".format(mae_overall))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SrvsF6Ailrud"},"outputs":[],"source":["# forecast skill for sunny and cloudy days individually\n","## calculate the rmse of persistent model\n","from Relative_op_func import *\n","\n","ntimes = times_test.shape[0]\n","per_prediction = np.zeros(ntimes)\n","for i in range(ntimes):\n","    CSI_cur, P_theo_cur = Relative_output(times_test[i],pv_log_test[i][0])\n","    CSI, P_theo_pred = Relative_output(times_test[i]+datetime.timedelta(minutes=15),pv_pred_test[i])\n","    per_prediction[i] = CSI_cur*P_theo_pred\n","\n","per_prediction_sunny = per_prediction[mask]\n","per_prediction_cloudy = per_prediction[~mask]\n","\n","per_rmse_sunny = np.sqrt(np.mean(np.square((per_prediction_sunny-pv_pred_test_sunny))))\n","per_rmse_cloudy = np.sqrt(np.mean(np.square((per_prediction_cloudy-pv_pred_test_cloudy))))\n","per_rmse_overall = np.sqrt((per_rmse_sunny**2*len(pv_pred_test_sunny)+per_rmse_cloudy**2*len(pv_pred_test_cloudy))/(len(pv_pred_test)))\n","print(\"test set sunny days RMSE persistence: {0:.3f}\".format(per_rmse_sunny))\n","print(\"test set cloudy days RMSE persistence: {0:.3f}\".format(per_rmse_cloudy))\n","print(\"test set overall RMSE persistence: {0:.3f}\".format(per_rmse_overall))\n","\n","## calculate the forecasting skills\n","forecast_skill_sunny = 1 - rmse_sunny/per_rmse_sunny\n","forecast_skill_cloudy = 1 - rmse_cloudy/per_rmse_cloudy\n","forecast_skill_overall = 1 - rmse_overall/per_rmse_overall\n","print(\"test set sunny days forecast skill: {0:.2f}%\".format(forecast_skill_sunny*100))\n","print(\"test set cloudy days forecast skill: {0:.2f}%\".format(forecast_skill_cloudy*100))\n","print(\"test set overall forecast skill: {0:.2f}%\".format(forecast_skill_overall*100))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pBG3gu-wlrue"},"outputs":[],"source":["# visualization of forecast predictions\n","dates_test = np.array([dtinfo.date() for dtinfo in times_test])\n","hours_test = np.array([dtinfo.time() for dtinfo in times_test])\n","\n","f,axarr = plt.subplots(10,2,sharex=False, sharey = True)\n","xfmt = mdates.DateFormatter('%H')\n","fmt_date = datetime.date(2000,1,1)\n","\n","green = '#8AB8A7'\n","red = '#B83A4B'\n","blue = '#67AFD2'\n","grey =  '#B6B1A9'\n","\n","for i,date in enumerate(sunny_dates_test):\n","    ax = axarr[i,0]\n","    date_mask = (dates_test == date)\n","    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]] \n","    \n","    rmse = np.sqrt(np.mean(np.square((pv_pred_test[date_mask]-prediction_ensemble[date_mask]))))\n","    mae = np.mean(np.abs((pv_pred_test[date_mask]-prediction_ensemble[date_mask])))\n","    per_rmse = np.sqrt(np.mean(np.square((pv_pred_test[date_mask]-per_prediction[date_mask]))))\n","    fs = (1 - rmse/per_rmse)*100\n","    \n","    ax.plot(hours_xaxis, pv_pred_test[date_mask], linewidth = 1,color=grey)\n","    ax.fill_between(hours_xaxis, 0, pv_pred_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n","    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth = 1,label = 'SUNSET forecast',color=red,markerfacecolor=\"None\")\n","    ax.set_ylabel('PV output (kW)')\n","    ax.xaxis.set_major_formatter(xfmt)\n","    ax.text(0.75,0.85,'Sunny_'+str(i+1), transform=ax.transAxes)\n","    ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\\nFS: {2:.2f}%\".format(rmse,mae,fs),transform=ax.transAxes)\n","\n","for i,date in enumerate(cloudy_dates_test):\n","    ax = axarr[i,1]\n","    date_mask = (dates_test == date)\n","    hours_xaxis= [datetime.datetime.combine(fmt_date, hour) for hour in hours_test[date_mask]] \n","    \n","    rmse = np.sqrt(np.mean(np.square((pv_pred_test[date_mask]-prediction_ensemble[date_mask]))))\n","    mae = np.mean(np.abs((pv_pred_test[date_mask]-prediction_ensemble[date_mask])))\n","    per_rmse = np.sqrt(np.mean(np.square((pv_pred_test[date_mask]-per_prediction[date_mask]))))\n","    fs = (1 - rmse/per_rmse)*100\n","    \n","    ax.plot(hours_xaxis, pv_pred_test[date_mask], linewidth = 1,color=grey)\n","    ax.fill_between(hours_xaxis, 0, pv_pred_test[date_mask], color=grey, alpha=.2, label = 'Ground truth')\n","    ax.plot(hours_xaxis, prediction_ensemble[date_mask],linewidth = 1,label = 'SUNSET forecast',color=red,markerfacecolor=\"None\")\n","    ax.set_ylabel('PV output (kW)')\n","    ax.xaxis.set_major_formatter(xfmt)\n","    ax.text(0.75,0.85,'Cloudy_'+str(i+1), transform=ax.transAxes)\n","    ax.text(0.05,0.65,\"RMSE: {0:.2f}\\nMAE: {1:.2f}\\nFS: {2:.2f}%\".format(rmse,mae,fs),transform=ax.transAxes)\n","\n","    \n","axarr[0,0].set_ylim(0, 30)\n","axarr[0,0].legend(bbox_to_anchor= [1.15,1.3], loc = 'upper center', ncol = 3)\n","axarr[-1,0].set_xlabel('Hour of day')\n","axarr[-1,1].set_xlabel('Hour of day')\n","\n","f.set_size_inches(10,25)    \n","plt.show()  "]}],"metadata":{"kernelspec":{"display_name":"Python 3.6.8 ('SKIPPD_venv')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"vscode":{"interpreter":{"hash":"074c471e510a42acbf43893a6d8078cb5a0d55f128880afca4e36b6d6e9c9498"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}